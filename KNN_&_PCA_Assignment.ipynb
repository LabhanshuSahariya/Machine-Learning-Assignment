{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Que 1. What is K-Nearest Neighbors (KNN) and how does it work in both\n",
        "classification and regression problems?\n",
        "- K-Nearest Neighbors (KNN) is a simple, supervised algorithm that classifies or predicts a new data point by looking at the majority class or average value of its 'K' closest training data points, using distance metrics to define \"closeness\" without building a complex model, making it a \"lazy\" and non-parametric learner.\n",
        "\n",
        "- In classification, it uses a majority vote & in regression, it takes the mean of neighbor values, often weighting closer points more heavily.\n"
      ],
      "metadata": {
        "id": "pceU0Q5YGNqW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2: What is the Curse of Dimensionality and how does it affect KNN\n",
        "performance?\n",
        "- As dimensions (features) increase, the volume of the feature space grows exponentially, making data points very spread out (sparse). In high dimensions, the difference between the closest and farthest points to a query point diminishes; all points seem almost equidistant, making distance metrics less useful, making it hard to find truly representative neighbors.\n",
        "\n",
        "- **How it Affects KNN Performance :**\n",
        "    - **Loss of Locality:** The concept of \"nearest\" becomes unreliable because points that seem close in many dimensions might actually be far apart in a meaningful sense.\n",
        "    \n",
        "    - **Increased Data Requirement:** To effectively sample the sparse high-dimensional space and find reliable neighbors, KNN needs exponentially more data, often becoming computationally infeasible.\n",
        "    - **Overfitting & Noise:** With many irrelevant features, it's harder to distinguish signal from noise, leading KNN to focus on irrelevant dimensions, causing poor generalization and overfitting.\n",
        "    - **Computational Burden:** Calculating distances to all neighbors becomes computationally expensive as dimensions and data size grow, increasing processing time."
      ],
      "metadata": {
        "id": "zuCsFAk8HGXh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Que 3. What is Principal Component Analysis (PCA)? How is it different from feature selection?\n",
        "- Principal Component Analysis (PCA) is a feature extraction method that transforms correlated original features into fewer, uncorrelated \"artificial\" features called Principal Components (PCs), maximizing variance capture for dimensionality reduction, while Feature Selection directly picks a subset of the original features, evaluating their predictive power for a target, without creating new ones. PCA creates new, combined features (less interpretable) to reduce dimensions, whereas Feature Selection chooses the best existing features (more interpretable)."
      ],
      "metadata": {
        "id": "QOH0vjQtI23E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 4: What are eigenvalues and eigenvectors in PCA, and why are they\n",
        "important?\n",
        "- **Eigenvectors:** These are the directions in the data space where the data varies the most. Each eigenvector is a new axis (a principal component).\n",
        "\n",
        "- **Eigenvalues:** These are scalar values associated with each eigenvector, representing the magnitude of variance along that eigenvector's direction. A larger eigenvalue means more variance, thus more information, is captured.\n",
        "- **Why They Are Important in PCA:**\n",
        "\n",
        "    - **Dimensionality Reduction:** By sorting eigenvectors by their eigenvalues, we find the most important directions (principal components) that capture the bulk of the data's variability. We can then discard components with small eigenvalues, reducing dimensions while minimizing information loss.\n",
        "    \n",
        "    - **Data Transformation:** They transform the data into a new, lower-dimensional space where features are uncorrelated (orthogonal) and ordered by importance.\n",
        "    - **Identifying Key Patterns:** They reveal the underlying structure and most significant patterns in the data, separating signal (high variance) from noise (low variance).\n",
        "    - **Data Compression & Visualization:** They enable efficient data compression and make high-dimensional data easier to visualize and process for machine learning models."
      ],
      "metadata": {
        "id": "g4c-ClPnJlTZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5: How do KNN and PCA complement each other when applied in a single pipeline?\n",
        "- When applied in a single pipeline, PCA and KNN complement each other by addressing each other's limitations. PCA (Principal Component Analysis) reduces data complexity, which directly enhances the speed and accuracy of the distance-based KNN (K-Nearest Neighbors) algorithm.\n",
        "\n",
        "- **Here is how they complement each other:**\n",
        "\n",
        "    - **Mitigating the \"Curse of Dimensionality\":** KNN relies on distance metrics (Euclidean, etc.) to find the closest data points. In high-dimensional spaces, distances become less meaningful. PCA reduces the feature space while retaining maximum variance, which helps KNN find more relevant neighbors.\n",
        "    \n",
        "    - **Improving Computational Efficiency:** KNN is computationally expensive (lazy learner) because it calculates the distance between the test point and all training points. By reducing the number of dimensions, PCA significantly speeds up the training and inference time for KNN.\n",
        "    - **Noise Reduction and Improved Accuracy:** Original data often contains noisy or redundant features that can mislead KNN. PCA filters out these components, often increasing the classification accuracy, sometimes by as much as 7.5% or more compared to KNN alone.\n",
        "    - **Eliminating Multicollinearity:** KNN assumes features are independent. PCA transforms correlated features into a smaller set of uncorrelated components, allowing for more stable, reliable distance calculations.\n"
      ],
      "metadata": {
        "id": "fum3VNdnLZ06"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I-PuHEXIGEgf",
        "outputId": "32e6adaa-343f-4aac-ce44-8701957e00a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy Without Scaling : 0.7407407407407407\n",
            "Accuracy With Scaling : 0.9629629629629629\n"
          ]
        }
      ],
      "source": [
        "# Que 6. Train a KNN Classifier on the Wine dataset with and without feature scaling.\n",
        "# Compare model accuracy in both cases.\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "y = data.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)\n",
        "clf = KNeighborsClassifier()\n",
        "clf.fit(X_train, y_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "print(f'Accuracy Without Scaling : {accuracy_score(y_test, y_pred)}')\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "clf = KNeighborsClassifier()\n",
        "clf.fit(X_train, y_train)\n",
        "y_pred_scaled = clf.predict(X_test)\n",
        "print(f'Accuracy With Scaling : {accuracy_score(y_test, y_pred_scaled)}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Que 7. Train a PCA model on the Wine dataset and\n",
        "# print the explained variance ratio of each principal component.\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "y = data.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "pca = PCA()\n",
        "X_train = pca.fit_transform(X_train)\n",
        "X_test = pca.transform(X_test)\n",
        "\n",
        "pca.explained_variance_ratio_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HJJzIVBjN62v",
        "outputId": "a4ad7fa5-df94-48a3-a10b-e3c23e0a79df"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.36196226, 0.18763862, 0.11656548, 0.07578973, 0.07043753,\n",
              "       0.04552517, 0.03584257, 0.02646315, 0.02174942, 0.01958347,\n",
              "       0.01762321, 0.01323825, 0.00758114])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Que 8. Train a KNN Classifier on the PCA-transformed dataset (retain top 2 components).\n",
        "# Compare the accuracy with the original dataset.\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "y = data.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=2)\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "clf = KNeighborsClassifier()\n",
        "clf.fit(X_train_pca, y_train)\n",
        "y_pred_pca = clf.predict(X_test_pca)\n",
        "\n",
        "accuracy_pca = accuracy_score(y_test, y_pred_pca)\n",
        "print(f'Accuracy with KNN on 2 PCA Components: {accuracy_pca}')\n",
        "\n",
        "clf_scaled = KNeighborsClassifier()\n",
        "clf_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = clf_scaled.predict(X_test_scaled)\n",
        "print(f'Accuracy with KNN on Scaled Original Dataset : {accuracy_score(y_test, y_pred_scaled)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_4OtWiNFsLIC",
        "outputId": "ec09c0ad-5c19-4993-806f-3c6fe2a955aa"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with KNN on 2 PCA Components: 0.9814814814814815\n",
            "Accuracy with KNN on Scaled Original Dataset : 0.9629629629629629\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Que 9. Train a KNN Classifier with different distance metrics (euclidean, manhattan)\n",
        "# on the scaled Wine dataset and compare the results.\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "y = data.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "clf_euclidean = KNeighborsClassifier(metric='euclidean')\n",
        "clf_euclidean.fit(X_train_scaled, y_train)\n",
        "y_pred_euclidean = clf_euclidean.predict(X_test_scaled)\n",
        "print(f'Accuracy With KNN (Euclidean Distance) : {accuracy_score(y_test, y_pred_euclidean)}')\n",
        "\n",
        "clf_manhattan = KNeighborsClassifier(metric='manhattan')\n",
        "clf_manhattan.fit(X_train_scaled, y_train)\n",
        "y_pred_manhattan = clf_manhattan.predict(X_test_scaled)\n",
        "print(f'Accuracy With KNN (Manhattan Distance) : {accuracy_score(y_test, y_pred_manhattan)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-3-chAgtv0pn",
        "outputId": "9a5b731d-0391-4ca7-9427-eb4964469de1"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy With KNN (Euclidean Distance) : 0.9629629629629629\n",
            "Accuracy With KNN (Manhattan Distance) : 0.9629629629629629\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: You are working with a high-dimensional gene expression dataset to classify patients with different types of cancer.  \n",
        "Due to the large number of features and a small number of samples, traditional models overfit.  \n",
        "Explain how you would:\n",
        "1. Use PCA to reduce dimensionality\n",
        "2. Decide how many components to keep\n",
        "3. Use KNN for classification post-dimensionality reduction\n",
        "4. Evaluate the model\n",
        "5. Justify this pipeline to your stakeholders as a robust solution for real-world biomedical data\n",
        "\n",
        "**Answer**\n",
        "- To address the challenge of high-dimensional gene expression data causing overfitting in cancer classification, I would implement a Principal Component Analysis (PCA) and K-Nearest Neighbors (KNN) pipeline. This combination reduces noise and dimensionality while maintaining maximum variance, allowing for robust classification.\n",
        "\n",
        "- **1. PCA for Dimensionality Reduction :**\n",
        "    - **Preprocessing:** First, standardize the gene expression data (Z-score scaling) so that genes with higher absolute expression levels do not dominate the principal components.\n",
        "\n",
        "    - **Implementation:** Apply PCA to transform the thousands of correlated gene features into a new coordinate system of orthogonal, uncorrelated components (principal components - PCs), which are linear combinations of the original genes.\n",
        "    - **Effect:** The algorithm identifies directions (PCs) that capture the maximum variance in the data.\n",
        "\n",
        "- **2. Deciding Number of Components to Keep :**\n",
        "    - **Cumulative Variance Method:** I would calculate the cumulative explained variance ratio and select the smallest number of components that explain a high threshold of the variance, typically 90%–95%.\n",
        "\n",
        "    - **Scree Plot (Elbow Method):** I would plot the variance explained by each component and look for the \"elbow\" point—where the marginal gain in explained variance drops significantly—to select the optimal number of PCs.\n",
        "\n",
        "- **3. KNN for Classification Post-Reduction :**\n",
        "    - **Data Transformation:** Project the original standardized training and testing data onto the chosen \\(k\\) principal components.\n",
        "\n",
        "    - **KNN Application:** Feed these reduced components into the KNN algorithm. KNN computes the distance (e.g., Euclidean distance) between a new patient's sample and the labeled training samples.\n",
        "    - **Classification:** Assign the cancer type based on the majority vote of the \\(k\\) closest neighbors.\n",
        "\n",
        "- **4. Evaluating the Model :** Given the small sample size, I would use Nested Cross-Validation (e.g., 5-fold or Leave-One-Out) to prevent leakage during hyperparameter tuning (both for PCA components and \\(K\\) in KNN).\n",
        "\n",
        "    - **Metrics:** I would evaluate the model using Accuracy, Precision, Recall, and F1-Score.\n",
        "    - **Confusion Matrix:** Specifically check for false positives/negatives between cancer subtypes to ensure the model is robust.\n",
        "\n",
        "- **5. Justification to Stakeholders :**\n",
        "    - **Mitigates Overfitting:** By reducing thousands of genes to a few, high-variance components, the model stops \"learning\" noise and focuses on structural biological patterns.\n",
        "\n",
        "    - **Handles High-Dimensionality:** PCA solves the \"curse of dimensionality,\" making it ideal for microarrays where genes >> patient samples.\n",
        "    - **Computational Efficiency:** KNN, which can be slow on large feature sets, becomes much faster and more accurate on a lower-dimensional PCA-reduced dataset.\n",
        "    - **Interpretability:** PCA identifies \"metagenes\"—linear combinations of genes that contribute most to the variance—allowing clinicians to understand which underlying biological processes drive the classification."
      ],
      "metadata": {
        "id": "lCRHh9Uc04Gy"
      }
    }
  ]
}