{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Que 1. What is Information Gain, and how is it used in Decision Trees?\n",
        "- Information Gain (IG) measures how much a feature reduces uncertainty (entropy) in a dataset, quantifying the information it provides about the class label in decision trees, it's used to select the best attribute for splitting nodes, choosing the one with the highest IG to create the most informative tree structure, leading to purer subsets and faster convergence to leaf nodes.  \n",
        "    - At the root, calculate the Information Gain for every available feature.\n",
        "    \n",
        "    - The feature with the highest Information Gain is chosen as the root node because it best separates the data into purer groups.\n",
        "    - This process is repeated for subsequent nodes (branches). For each child node, the algorithm again finds the feature that offers the maximum IG for that specific subset of data.\n",
        "    - The tree grows by selecting features that maximally reduce uncertainty at each step, leading to leaf nodes that ideally contain instances of a single class."
      ],
      "metadata": {
        "id": "Kb3dwcNHvBHU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Que 2. What is the difference between Gini Impurity and Entropy?\n",
        "- Gini Impurity and Entropy are both impurity measures for decision trees, but Gini uses squared probabilities (faster, ranges 0-0.5), while Entropy uses logarithms (slower, ranges 0-1) to quantify disorder, with lower values indicating purer nodes. Gini is computationally cheaper and often preferred for large datasets, though Entropy can sometimes yield slightly better results by producing more balanced trees.\n",
        "\n",
        "-  Gini is often the default for large datasets due to speed, but both work well and give similar results.\n"
      ],
      "metadata": {
        "id": "EkFyVNKZwTFg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Que 3. What is Pre-Pruning in Decision Trees?\n",
        "- Pre-pruning (or early stopping) in decision trees stops the tree's growth during training, preventing it from becoming too complex and overfitting the data, by using criteria like setting a maximum tree depth, minimum samples per leaf, or minimum impurity decrease. It's more computationally efficient than post-pruning (which prunes a full tree later) but risks underfitting if stopped too early, a challenge known as the \"horizon effect\".  \n"
      ],
      "metadata": {
        "id": "kuZE7DIEyKL6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HLhKiV-iuot-",
        "outputId": "935ec0ba-73f2-469b-b1c2-85e50e9a875e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Importances (Gini Impurity):\n",
            "petal length (cm)    0.893264\n",
            "petal width (cm)     0.087626\n",
            "sepal width (cm)     0.019110\n",
            "sepal length (cm)    0.000000\n",
            "dtype: float64\n"
          ]
        }
      ],
      "source": [
        "# Que 4. Write a Python program to train a Decision Tree Classifier using Gini\n",
        "# Impurity as the criterion and print the feature importances (practical).\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "data = load_iris()\n",
        "df = pd.DataFrame(data.data, columns = data.feature_names)\n",
        "X = df\n",
        "y = data.target\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "classifier = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "classifier.fit(X_train, y_train)\n",
        "\n",
        "importances = classifier.feature_importances_\n",
        "\n",
        "feature_importances_series = pd.Series(importances, index=data.feature_names)\n",
        "feature_importances_series = feature_importances_series.sort_values(ascending=False)\n",
        "\n",
        "print(\"Feature Importances (Gini Impurity):\")\n",
        "print(feature_importances_series)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Que 5. What is a Support Vector Machine (SVM)?\n",
        "- A Support Vector Machine (SVM) is a powerful supervised machine learning algorithm used for classification and regression tasks. Its primary objective is to find the optimal decision boundary, known as a hyperplane, that maximally separates data points of different classes in a multi-dimensional space.\n",
        "- The fundamental idea behind an SVM involves several key components:\n",
        "    - **Hyperplane:** The decision boundary that separates different classes of data. In a two-dimensional space, this is a line; in a three-dimensional space, it is a plane; and in higher dimensions, it is a hyperplane.\n",
        "\n",
        "    - **Margin:** The distance between the decision boundary and the closest data points from each class. The SVM algorithm aims to maximize this distance to ensure the best possible separation and generalization to new data.\n",
        "    - **Support Vectors:** The data points that lie closest to the hyperplane and directly influence its position and orientation. Only these critical points are used to define the decision boundary, making the algorithm memory-efficient."
      ],
      "metadata": {
        "id": "HjpukqlX2pgL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Que 6. What is the Kernel Trick in SVM?\n",
        "- **Kernel Trick:** The Kernel Trick in SVM is a method to handle non-linear data by implicitly mapping it to a higher-dimensional space, making it linearly separable, without explicitly calculating the coordinates in that space, thus avoiding massive computation. Instead of transforming data points, a kernel function calculates the dot product (similarity) between data points in the new, higher dimension, allowing SVM to find a complex, non-linear decision boundary efficiently.\n"
      ],
      "metadata": {
        "id": "Iv9-tDHZ4GBt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Que 7. Write a Python program to train two SVM classifiers with Linear\n",
        "# and RBF kernels on the Wine dataset, then compare their accuracies.\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_wine\n",
        "data = load_wine()\n",
        "df = pd.DataFrame(data.data, columns = data.feature_names)\n",
        "X = df\n",
        "y = data.target\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=1)\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "clf1 = SVC(kernel='linear')\n",
        "clf1.fit(X_train, y_train)\n",
        "y_pred1 = clf1.predict(X_test)\n",
        "\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "print(\"Accuracies of SVC kernel linear :\\n\")\n",
        "print(f'accuracy_score : {accuracy_score(y_test, y_pred1)}')\n",
        "print(f'confusion_matrix :\\n {confusion_matrix(y_test, y_pred1)}')\n",
        "print(f'\\nclassification_report :\\n {classification_report(y_test, y_pred1)}')\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "clf2 = SVC(kernel='rbf')\n",
        "clf2.fit(X_train, y_train)\n",
        "y_pred2 = clf2.predict(X_test)\n",
        "\n",
        "print(\"Accuracies of SVC kernel RBF :\\n\")\n",
        "print(f'accuracy_score : {accuracy_score(y_test, y_pred2)}')\n",
        "print(f'confusion_matrix :\\n {confusion_matrix(y_test, y_pred2)}')\n",
        "print(f'\\nclassification_report :\\n {classification_report(y_test, y_pred2)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ob8LcFJe0k4W",
        "outputId": "c6d3d85a-2785-4622-aa45-f9dd9e5a4215"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracies of SVC kernel linear :\n",
            "\n",
            "accuracy_score : 0.9629629629629629\n",
            "confusion_matrix :\n",
            " [[23  0  0]\n",
            " [ 1 18  0]\n",
            " [ 0  1 11]]\n",
            "\n",
            "classification_report :\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      1.00      0.98        23\n",
            "           1       0.95      0.95      0.95        19\n",
            "           2       1.00      0.92      0.96        12\n",
            "\n",
            "    accuracy                           0.96        54\n",
            "   macro avg       0.97      0.95      0.96        54\n",
            "weighted avg       0.96      0.96      0.96        54\n",
            "\n",
            "Accuracies of SVC kernel RBF :\n",
            "\n",
            "accuracy_score : 0.6851851851851852\n",
            "confusion_matrix :\n",
            " [[19  0  4]\n",
            " [ 1 15  3]\n",
            " [ 0  9  3]]\n",
            "\n",
            "classification_report :\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.83      0.88        23\n",
            "           1       0.62      0.79      0.70        19\n",
            "           2       0.30      0.25      0.27        12\n",
            "\n",
            "    accuracy                           0.69        54\n",
            "   macro avg       0.62      0.62      0.62        54\n",
            "weighted avg       0.69      0.69      0.68        54\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Que 8. What is the Naive Bayes classifier, and why is it called Naive?\n",
        "- The Naive Bayes classifier is a simple yet powerful probabilistic machine learning algorithm, based on Bayes Theorem, that predicts class membership by calculating probabilities, assuming features are completely independent.\n",
        "- It's called \"Naive\" because it unrealistically assumes features don't affect each other, simplifying calculations but sometimes underperforming with real-world correlated data, yet it excels in text classification (spam filters, sentiment analysis) due to its speed and effectiveness.\n"
      ],
      "metadata": {
        "id": "JSZkCF2E-SMq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Que 9. Explain the differences between Gaussian Naive Bayes, Multinomial Naive Bayes, and Bernoulli Naive Bayes.\n",
        "- **Gaussian Naive Bayes (GNB):** When features are Continuous numerical, and follows normal distribution (e.g., height, weight, temperature). It calculates the mean and variance for each feature within each class and uses the Gaussian probability density function.\n",
        "    - Use Case: Predicting house prices based on size (continuous), medical diagnosis (e.g., blood pressure).  \n",
        "\n",
        "- **Multinomial Naive Bayes (MNB):** When inputs are text data (e.g., word counts in a document, number of times a word appears). It models the probability of observing counts of different features (words) for a given class.\n",
        "    - Use Case: Text classification (spam detection, topic modeling) based on word frequencies.\n",
        "\n",
        "- **Bernoulli Naive Bayes (BNB):** When features follows Bernoulli dustribution, Binary features (presence or absence, 0 or 1). It calculates the probability of a feature being present or absent (e.g., a word in a document).\n",
        "    - Use Case: Text classification where features are just whether a word is present or not (not its frequency)."
      ],
      "metadata": {
        "id": "e2orYjQh_UtL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 10: Breast Cancer Dataset\n",
        "# Write a Python program to train a Gaussian Naive Bayes classifier on the Breast Cancer dataset and evaluate accuracy.\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)\n",
        "\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "clf = GaussianNB()\n",
        "clf.fit(X_train, y_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "accuracy_score(y_test, y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OUv-VUv_5aUx",
        "outputId": "8f409cce-2da6-4d6b-e7e5-db8d75eb46f9"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9415204678362573"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    }
  ]
}